# Machine Learning: Overview

## Introduction

Machine Learning (ML) is a branch of artificial intelligence that
enables systems to learn from data and improve their performance over
time without being explicitly programmed. At its core, machine
learning algorithms aim to identify patterns in data and use those
patterns to make decisions or predictions.


Machine learning can be categorized into three main types: 
supervised learning, unsupervised learning, and reinforcement
learning. Each type differs in the data it uses and the learning tasks
it performs, addressing addresses different tasks and
problems. Supervised learning aims to predict outcomes based on
labeled data, unsupervised learning focuses on discovering hidden
patterns within the data, and reinforcement learning centers around
learning optimal actions through interaction with an environment.


Let's define some notations to introduce them:

- $X$: A set of feature vectors representing the input data. Each
  element $X_i$ corresponds to a set of features or attributes that
  describe an instance of data.
  
- $Y$: A set of labels or rewards associated with outcomes. In
  supervised learning, $Y$ is used to evaluate the correctness of the
  model’s predictions. In reinforcement learning, $Y$ represents the
  rewards that guide the learning process.
  
- $A$: A set of possible actions in a given context. In reinforcement
  learning, actions $A$ represent choices that can be made in response
  to a given situation, with the goal of maximizing a reward.

### Supervised Learning
Supervised learning is the most widely used type of machine
learning. In supervised learning, we have both feature vectors $X$ and
their corresponding labels $Y$. The objective is to train a model that
can predict $Y$ based on $X$. This model is trained on labeled
examples, where the correct outcome is known, and it adjusts its
internal parameters to minimize the error in its predictions, which
occurs as part of the cross-validation process.


Key tasks in supervised learning include:

- Classification: Assigning data points to predefined categories or classes.
- Regression: Predicting a continuous value based on input data.

In supervised learning, the data consists of both feature vectors $X$
and labels $Y$, namely, $(X, Y)$.


### Unsupervised Learning
Unsupervised learning involves learning patterns from data without any
associated labels or outcomes. The objective is to explore and
identify hidden structures in the feature vectors $X$. Since there are
no ground-truth labels $Y$ to guide the learning process, the
algorithm must discover patterns on its own. This is particularly useful
when subject matter experts are unsure of common properties within a
data set.



Common tasks in unsupervised learning include:

- Clustering: Grouping similar data points together based on certain
  features.
  
- Dimension Reduction: Simplifying the input data by reducing the
  number of features while preserving essential patterns.


In unsupervised learning, the data consists solely of feature vectors
$X$.



### Reinforcement Learning
Reinforcement learning involves learning how to make a sequence of
decisions to maximize a cumulative reward. Unlike supervised learning,
where the model learns from a static dataset of labeled examples,
reinforcement learning involves an agent that interacts with an
environment by taking actions $A$, receiving feedback in the form of
rewards $Y$, and learning over time which actions lead to the highest
cumulative reward.


The process in reinforcement learning involves:

- States: The context or environment the agent is in, represented by
  feature vectors $X$.

- Actions: The set of possible choices the agent can make in response
  to the current state, denoted as $A$.
  
- Rewards: Feedback the agent receives after taking an action, which
  guides the learning process.
  

In reinforcement learning, the data consists of feature vectors $X$,
actions $A$, and rewards $Y$, namely, $(X, A, Y)$.




#### What is Reinforcement Learning?

This following was written by Qianruo Tan. This is a detailed explanation of mandatory learning

- Reinforcement learning (RL) is a type of machine learning.
- Deep Q-Networks (DQN) 
- Agents learn by interacting with the environment.
- The goal of it is to maximize cumulative reward over time.
- Reinforcement Learning (RL) VS Supervised Learning (SL)

Supervised Learning (SL) and Reinforcement Learning (RL) share some
similarities in their frameworks, but they differ fundamentally in
how they learn and improve over time. In a supervised learning 
setting, training a neural network, such as learning to play a game,
requires a substantial dataset with labeled examples. This involves
recording the gameplay process, including inputs like key presses
and even eye movements. 

The network learns by observing how certain actions (inputs) lead to
specific outcomes (labels), essentially mimicking human actions to
predict what to do next. However, it only imitates what it's been
taught and does not learn to improve on its own beyond the data it's
given.

In contrast, reinforcement learning does not rely on a predefined
dataset with target labels. Instead, it uses a policy network that
transforms input frames from the game into actions. The training
process in RL starts with a network that has no prior knowledge. It
receives input frames directly from the game environment and must
figure out what actions to take to maximize rewards over time. The
simplest way to train this policy network is through policy
gradients, where the model learns by interacting with the
environment and receiving feedback. 

However, RL has a downside: if the model encounters a failure, it
might incorrectly generalize that a certain strategy is bad, leading
to what's known as the credit assignment problem—it struggles to
properly attribute which actions led to success or failure. This
means the network may become overly cautious, reducing its
exploration of potentially good strategies that initially resulted
in failure.



#### What does this note contains

- [Actual usages & Scopes & Limitations](#actual-usages--scopes--limitations)
- [Q-learning](#q-learning)
- [An example of simple models](#grid)
- [Rewards and Penalties](#rewards-and-penalties)




#### Actual usages & Scopes & Limitations

##### Actual usages

- Gaming and Simulations (AlphaGo)
- Cooperate with Bayesian Optimization
- Robotics and Automation
- Self-driving Cars
- Finance and Trading
- Personalization and Recommendations


##### Scopes & Limitations

- **Reinforcement Learning vs. Evolutionary Methods**

Evolutionary methods cannot utilize real-time feedback from actions,
making them less efficient in dynamic environments where immediate
learning is advantageous.

- **Policy Gradient Methods**

Unlike evolutionary methods, policy gradients interact with the
environment to improve performance, allowing more efficient use of
detailed feedback from individual interactions.

- **Misunderstanding of Optimization**

Optimization in RL is about improving performance incrementally, not
guaranteeing the best possible outcome in every scenario.




#### Q-Learning


##### Q-learning Overview

- Aims to learn the optimal action-value function $( q_*(s, a) )$,
regardless of the policy being followed during learning. 
- The main objective is to approximate this function through a
learning process where the agent interacts with its environment,
receiving rewards and updating its knowledge of state-action pairs.


##### Q-learning Update Rule
The core formula of Q-learning is based on the Bellman equation for
the action-value function:

**$$ [Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \right]] $$**


- $( Q(s, a) )$: The current Q-value for taking action $( a )$ in
state $( s )$.

- $( \alpha )$: The learning rate, set to 0.1 in your code, which
controls how much new information overrides the old information.

- $( R )$: The reward received after taking action $( a )$ in state
$( s )$.

- $( \gamma )$: The discount factor, set to 0.9 in your code, which
determines the importance of future rewards.

- $( \max_{a'} Q(s', a') )$: The maximum Q-value for the next state
$( s' )$ across all possible actions $( a' )$. This term represents
the best possible future reward from the next state.

- $( Q(s, a) )$ (the initial term on the right side): The old
Q-value for the current state-action pair, which is being updated.


##### Key Components of Q-learning

**Off-policy Learning**:

- Q-learning is an off-policy method, meaning the agent can learn
the optimal policy independently. Used to select actions during
training (e.g., $( \epsilon )$-greedy strategy).


##### Common supporting tools 

**Environment and State Management**: 

```python
Functions get_next_state() and get_reward()
```

**Q-Table Initialization**: 

```python
q_table = np.zeros((grid_size, grid_size, 4)) 
```


##### Common supporting tools 

**Neural Network**: A computational model inspired by the human
brain, consisting of layers of interconnected nodes (neurons). It
learns to recognize patterns in data by adjusting weights through
multiple training iterations.

**Deep Q-Learning (DQN)**: An algorithm that combines Q-Learning
with deep neural networks to approximate the Q-value function,
allowing the agent to choose optimal actions in complex environments
with high-dimensional state spaces.

**Policy Network**: A neural network designed to output the best
action to take in a given state. It maps input states directly to
actions, enabling the agent to decide what to do without relying on
a value function.

**Policy Gradient**: An optimization technique in reinforcement
learning where the agent learns the best policy by directly
adjusting its parameters to maximize the cumulative reward, rather
than estimating value functions.


##### Common supporting tools 

**Behavior Policy**: The strategy that the agent uses to explore the
environment and collect experiences. It often includes some
randomness to encourage exploration of different actions.

**Target Policy**: The policy that the agent is trying to optimize.
In some algorithms, like Q-learning, it is used to determine the
best action to take based on learned values.

**Epsilon-Greedy Policy**: A strategy used to balance exploration
and exploitation. With a small probability (epsilon), the agent
chooses a random action to explore, and with the remaining
probability (1 - epsilon), it chooses the best-known action based
on current knowledge.




#### How to use it?


##### Grid

The environment is a 4×4 grid. 
The agent starts in (0,0) and aims to reach (3,3).


##### Environment Setup

The following chunk is used to set up the environment, which the agent
 is in. I also set up a seed, for the reproduction.
```{python}
## Import packages
import numpy as np
import random

## Set the random seed for reproducibility
random.seed(3255)
np.random.seed(3255)

## Define the environment
grid_size = 4
start_state = (0, 0)
goal_state = (3, 3)
obstacles = []
```

##### Hyperparameters

The following chunk is used to set key reinforcement learning 
hyperparameters, including the learning rate, discount factor, 
exploration rate, and the number of training episodes.
```{python}
## Define the hyperparameters for our value function
alpha = 0.1  # Learning rate
gamma = 0.9  # Discount factor
epsilon = 0.2  # Exploration rate
num_episodes = 1000  # Try 1000 times
```

##### Q-table Initialization

The following chunk initializes a Q-table with zeros to store the 
value estimates for each state-action pair in a grid, and defines 
the available actions (up, down, left, right) for the agent to choose from.
```{python}
## Initialize the Q-table
q_table = np.zeros((grid_size, grid_size, 4))  
## 4 possible actions: up, down, left, right
## The output Q-table follows thie 4 directions

## Define the action space
actions = ["up", "down", "left", "right"]
```

##### State Transitions and Reward Calculation

The following chunk defines functions to determine the agent’s next 
state based on an action (preventing it from leaving the grid 
boundaries) and to assign a numerical reward depending on whether 
the state is a goal, an obstacle, or a general location.
```{python}
def get_next_state(state, action):
    i, j = state
    if action == "up":
        return (max(i - 1, 0), j)
    elif action == "down":
        return (min(i + 1, grid_size - 1), j)
    elif action == "left":
        return (i, max(j - 1, 0))
    elif action == "right":
        return (i, min(j + 1, grid_size - 1))
    
def get_reward(state):
    if state == goal_state:
        return 10
    elif state in obstacles:
        return -5
    else:
        return -1
```

Move up: decrease row index (i) by 1, but don't go out of bounds
(minimum row index is 0)
Move down: increase row index (i) by 1, but don't go out of bounds
(maximum row index is grid_size - 1)
Move left: decrease column index (j) by 1, but don't go out of bounds
(minimum column index is 0)
Move right: increase column index (j) by 1, but don't go out of
bounds (maximum column index is grid_size - 1)

##### Action Selection (Epsilon-Greedy Policy)

The following chunk implements an epsilon-greedy strategy for action 
selection, randomly exploring with probability epsilon or exploiting 
the action with the highest Q-value otherwise.
```{python}
def choose_action(state):
    ## Epsilon-greedy action selection
    if random.uniform(0, 1) < epsilon:
        return random.choice(actions)  # Explore
    else:
        ## Exploit (choose the action with the highest Q-value)
        state_q_values = q_table[state[0], state[1], :]
        return actions[np.argmax(state_q_values)]
```

state represents the agent's current position on the grid (given as
a tuple (i, j)). And there are 4 x 4 = 16 possibilities.
The function uses this state to decide whether to explore (choose a
random action) or exploit (choose the best-known action based on the 
Q-table).
The Q-values for that particular state are retrieved using q_table
[state[0], state[1], :].


##### Q-L Algorithm (Main Loop)

The following chunk runs the Q-learning algorithm over multiple 
episodes, choosing actions based on an epsilon-greedy policy, observing 
the resulting reward and next state, then updating the Q-values using the 
Q-learning equation until it reaches the goal state.
```{python}
## Q-learning Algorithm
for episode in range(num_episodes):
    state = start_state
    while state != goal_state:
        action = choose_action(state)
        next_state = get_next_state(state, action)
        
        ## Get the reward for moving to the next state
        reward = get_reward(next_state)
        
        ## Update Q-value using the Q-learning formula
        current_q_value = q_table[
            state[0], state[1], actions.index(action)
            ]
        max_future_q_value = np.max(
            q_table[next_state[0], next_state[1], :]
            )
        new_q_value = current_q_value + alpha * (
            reward + gamma * max_future_q_value - current_q_value
            )
        q_table[state[0], state[1], actions.index(action)] = new_q_value
        
        state = next_state
```

state is initially set to start_state (e.g., (0, 0)).
Within the loop, the choose_action(state) function is called to
select an action based on the agent's current position.
The agent then moves to a new next_state using the chosen action,
and the current state is updated to next_state.
Throughout this loop, state always represents the agent’s current
position on the grid as it progresses toward the goal_state.


##### Result

The following chunk prints out the final learned Q-values, showing 
the agent's estimated values for each state-action pair after training.
```{python}
## Display the learned Q-values
print("Learned Q-Table:")
print(q_table)
```

This is the directly output of this example, there are three layers
of bracket, each of them have different meanings.
First layer of brackets: Represents the rows of the grid. Each layer
represents a row in the Q-table(i.e., a row position in the qrid
environment).
Second layer of brackets: Represents the columns of the grid. Each
subarray represents a specificstate in that row (i.e., a specific
position in the qrid, such as (0,0), (1,1), etc.).
Third layer of brackets: Represents the Q-values for each action in
that state. Each elementrepresents the Q-value of a specific action
in that state (e.g. the four actions: up, down, left, right)


##### Route visualization

The following chunk visualizes the learned policy on a grid by 
drawing arrows in each cell to indicate the best action according 
to the Q-table, while highlighting the start, goal, and obstacle 
positions.
```{python}
import matplotlib.pyplot as plt

# Define directional arrows for actions: up, down, left, right
directions = {0: '↑', 1: '↓', 2: '←', 3: '→'}

# Visualization of the best actions on the grid
fig, ax = plt.subplots(figsize=(6, 6))
ax.set_xticks(np.arange(0.5, grid_size, 1))
ax.set_yticks(np.arange(0.5, grid_size, 1))
ax.grid(True, which='both')
ax.set_xticklabels([])
ax.set_yticklabels([])

# Highlight Start, Goal, and Obstacles
ax.add_patch(
    plt.Rectangle(
        start_state, 1, 1, fill=True, color='yellow', alpha=0.3
        )
    )
ax.add_patch(
    plt.Rectangle(
        goal_state, 1, 1, fill=True, color='green', alpha=0.3
        )
    )

# Highlight obstacles in red
for obstacle in obstacles:
    ax.add_patch(
        plt.Rectangle(
            obstacle, 1, 1, fill=True, color='red', alpha=0.5
            )
        )

# Add arrows (text-based) for the best actions
for i in range(grid_size):
    for j in range(grid_size):
        if (i, j) in obstacles:
            continue  # Skip drawing arrows for obstacle locations
        q_values = q_table[i, j, :]
        best_action_idx = np.argmax(q_values)
        best_action = directions[best_action_idx]  
        # Text-based direction arrows (↑, ↓, ←, →)

        # Adding text-based arrows at the grid cells
        ax.text(
            j + 0.5, i + 0.5, best_action, ha='center', va='center', fontsize=20
            )

# Draw grid lines and adjust axis
plt.xlim(0, grid_size)
plt.ylim(0, grid_size)
plt.gca().invert_yaxis()  
# Invert Y-axis to display it in the correct orientation
plt.show()
```


##### Grid with obstacles

Because of the reusing and stating cause the output become exactly
the same. I rewrote the example with obstacles.
```{python}
import numpy as np
import matplotlib.pyplot as plt

## Set random seed for reproducibility
random.seed(3255)
np.random.seed(3255)

## Define the environment
grid_size = 4
start_state = (0, 0)
goal_state = (3, 3)
obstacles = [(1, 1), (2, 2)]  # Ensure obstacles are unique

## Define directions for visualization
directions = {0: '↑', 1: '↓', 2: '←', 3: '→'}
```

##### Run the grid with obstacles

The following defines reward and next-state functions that incorporate obstacles and goals, and then visualizes the learned Q-values on a grid by marking start, goal, and obstacle cells, as well as displaying the agent’s best action in each non-obstacle cell using directional arrows.
```{python}
## Function to get reward
def get_reward(state):
    if state == goal_state:
        return 10
    elif state in obstacles:
        return -5
    else:
        return -1

## Function to get the next state based on the action
def get_next_state(state, action):
    i, j = state
    if action == "up":
        next_state = (max(i - 1, 0), j)
    elif action == "down":
        next_state = (min(i + 1, grid_size - 1), j)
    elif action == "left":
        next_state = (i, max(j - 1, 0))
    elif action == "right":
        next_state = (i, min(j + 1, grid_size - 1))
    
    ## Prevent moving into obstacles
    if next_state in obstacles:
        return state  
        # Stay in the same position if the next state is an obstacle
    else:
        return next_state

## Visualization function
def plot_grid_with_obstacles():
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.clear()  # Clear the plot to avoid overlap
    ax.set_xticks(np.arange(0.5, grid_size, 1))
    ax.set_yticks(np.arange(0.5, grid_size, 1))
    ax.grid(True, which='both')
    ax.set_xticklabels([])
    ax.set_yticklabels([])

    ## Highlight Start, Goal, and Obstacles
    ax.add_patch(
        plt.Rectangle(
            start_state, 1, 1, fill=True, color='yellow', alpha=0.3
            )
        )
    ax.add_patch(
        plt.Rectangle(
            goal_state, 1, 1, fill=True, color='green', alpha=0.3
            )
        )

    ## Highlight obstacles in red
    for obstacle in set(obstacles):  # Use a set to ensure uniqueness
        ax.add_patch(
            plt.Rectangle(
                obstacle, 1, 1, fill=True, color='red', alpha=0.5
                )
            )

    ## Add arrows for the best actions from Q-table
    for i in range(grid_size):
        for j in range(grid_size):
            if (i, j) in obstacles:
                continue  # Skip arrows for obstacle locations
            q_values = q_table[i, j, :]
            best_action_idx = np.argmax(q_values)
            best_action = directions[best_action_idx]  
            # Directional arrows (↑, ↓, ←, →)
            ax.text(
                j + 0.5, i + 0.5, best_action, ha='center', va='center', fontsize=20
                )

    ## Draw grid lines and adjust axis
    plt.xlim(0, grid_size)
    plt.ylim(0, grid_size)
    plt.gca().invert_yaxis()  
    # Invert Y-axis for correct orientation
    plt.show()

## Call the visualization function
plot_grid_with_obstacles()
```




##### Rewards and Penalties

The following chunk trains a Q-learning agent over multiple episodes, updating Q-values as it navigates from a start state to a goal state, stores the cumulative reward obtained in each episode, and finally plots how the total earned reward per episode evolves over time.
```python
## Initialize list to store cumulative rewards for each episode
cumulative_rewards = []

for episode in range(num_episodes):
    state = start_state
    episode_reward = 0  # Track total reward for the current episode
    
    while state != goal_state:
        action = choose_action(state)
        next_state = get_next_state(state, action)
        
        reward = get_reward(next_state)
        episode_reward += reward  # Accumulate reward for this episode
        
        # Update Q-value
        current_q_value = q_table[
            state[0], state[1], actions.index(action)
            ]
        max_future_q_value = np.max(
            q_table[next_state[0], next_state[1], :]
            )
        new_q_value = current_q_value + alpha * 
                        (
                            reward + gamma * max_future_q_value - current_q_value
                            )
        q_table[state[0], state[1], actions.index(action)] = new_q_value
        
        state = next_state  # Move to the next state
    
    cumulative_rewards.append(episode_reward)  
    # Store cumulative reward for this episode

## Visualization of Cumulative Rewards
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(range(num_episodes), cumulative_rewards, 
            label='Cumulative Reward per Episode')
plt.xlabel("Episode")
plt.ylabel("Cumulative Reward")
plt.title("Cumulative Rewards and Penalties Over Episodes")
plt.legend()
plt.show()
```


##### Track Cumulative Rewards Over Episodes

The following chunk trains a Q-learning agent by repeatedly choosing actions, updating its Q-values based on observed rewards, recording total rewards per episode, and then visualizes how cumulative rewards evolve over the episodes.
```{python}
## Initialize list to store cumulative rewards for each episode
cumulative_rewards = []

for episode in range(num_episodes):
    state = start_state
    episode_reward = 0  # Track total reward for the current episode
    
    while state != goal_state:
        action = choose_action(state)
        next_state = get_next_state(state, action)
        
        reward = get_reward(next_state)
        episode_reward += reward  # Accumulate reward for this episode
        
        # Update Q-value
        current_q_value = q_table[
            state[0], state[1], actions.index(action)
            ]
        max_future_q_value = np.max(
            q_table[next_state[0], next_state[1], :]
            )
        new_q_value = current_q_value + alpha * (
            reward + gamma * max_future_q_value - current_q_value
            )
        q_table[state[0], state[1], actions.index(action)] = new_q_value
        
        state = next_state  # Move to the next state
    
    cumulative_rewards.append(episode_reward)  
    # Store cumulative reward for this episode

## Visualization of Cumulative Rewards
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(
    range(num_episodes), cumulative_rewards, label='Cumulative Reward per Episode'
    )
plt.xlabel("Episode")
plt.ylabel("Cumulative Reward")
plt.title("Cumulative Rewards and Penalties Over Episodes")
plt.legend()
plt.show()
```


This is the end of Qianruo's session.



## Demonstration: Tic-Tac-Toe 

In this demonstration, we'll develop a reinforcement learning agent
that learns to play Tic-Tac-Toe using the Q-learning algorithm. We'll
start with an overview of the work plan and then present the code step
by step, explaining each part in detail.



### Work Plan Overview

1. **Import Required Libraries**: Import necessary Python libraries for the implementation.

2. **Define the Default Q-Value Function**: Create a function to initialize default Q-values for unseen states.

3. **Implement the `TicTacToe` Game Class**: Define the game environment, including the board, moves, and win conditions.

4. **Implement the `QLearningAgent` Class**: Develop the agent that will learn optimal strategies using Q-learning.

5. **Define the Game Playing Function**: Write a function to simulate games between the agent and an opponent.

6. **Define the Training Function**: Create a function to train the agent over multiple episodes.

7. **Define the Evaluation Function**: Assess the agent's performance after training.

8. **Enable Human Interaction**: Allow a human player to play against the trained agent.

9. **Main Function**: Tie all components together and provide a user interface.

### Code Implementation

Let's go through the code step by step.

#### Import Required Libraries

We start by importing the necessary libraries.

```{python}
import numpy as np
import random
from collections import defaultdict
```

#### Define the Default Q-Value Function

We define a function that returns a NumPy array of zeros, which
initializes the Q-values for new states.

```{python}
def default_q_value():
    return np.zeros(9)
```

This function ensures that every new state encountered by the agent
has an initial Q-value of zero for all possible actions.


#### Implement the `TicTacToe` Game Class

We create a class to represent the Tic-Tac-Toe game environment.
```{python}
class TicTacToe:
    def __init__(self):
        self.board = [' '] * 9
        self.current_winner = None

    def reset(self):
        self.board = [' '] * 9
        self.current_winner = None
        return self.get_state()

    def available_actions(self):
        return [i for i, spot in enumerate(self.board) if spot == ' ']

    def get_state(self):
        return tuple(self.board)

    def make_move(self, square, letter):
        if self.board[square] == ' ':
            self.board[square] = letter
            if self.winner(square, letter):
                self.current_winner = letter
            return True
        return False

    def winner(self, square, letter):
        # Check rows, columns, and diagonals for a win
        row_ind = square // 3
        row = self.board[row_ind*3:(row_ind+1)*3]
        if all(s == letter for s in row):
            return True

        col_ind = square % 3
        col = [self.board[col_ind+i*3] for i in range(3)]
        if all(s == letter for s in col):
            return True

        # Check diagonals
        if square % 2 == 0:
            diag1 = [self.board[i] for i in [0,4,8]]
            if all(s == letter for s in diag1):
                return True
            diag2 = [self.board[i] for i in [2,4,6]]
            if all(s == letter for s in diag2):
                return True

        return False

    def is_full(self):
        return ' ' not in self.board

    def print_board(self):
        # Helper function to print the board
        for row in [self.board[i*3:(i+1)*3] for i in range(3)]:
            print('| ' + ' | '.join(row) + ' |')

    def print_board_nums(self):
        # Helper function to show the number mapping to board positions
        number_board = [str(i) for i in range(9)]
        for row in [number_board[i*3:(i+1)*3] for i in range(3)]:
            print('| ' + ' | '.join(row) + ' |')
```


Explanation:

+ `__init__`: Initializes the game board and sets the current winner to `None`.
+ `reset`: Resets the board for a new game and returns the initial state.
+ `available_actions`: Returns a list of indices where moves can be
  made.
+ `get_state`: Returns a tuple representing the current state of the
  board.
+ `make_move`: Places a letter ('X' or 'O') on the board if the move
  is valid.
+ `winner`: Checks if the last move resulted in a win.
+ `is_full`: Checks if the board is full, indicating a draw.
+ `print_board` and `print_board_nums`: Helper methods to display the
  board and the numbering for positions.
  
  
#### Implement the QLearningAgent Class

We define a class for the agent that will learn using Q-learning.
```{python}
class QLearningAgent:
    def __init__(self, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.9995):
        self.q_table = defaultdict(default_q_value)
        self.alpha = alpha          # Learning rate
        self.gamma = gamma          # Discount factor
        self.epsilon = epsilon      # Exploration rate
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = 0.01     # Minimum exploration rate

    def choose_action(self, state, available_actions):
        # ε-greedy action selection
        if np.random.rand() < self.epsilon:
            return random.choice(available_actions)
        else:
            state_values = self.q_table[state]
            # Select action with highest Q-value among available actions
            q_values = [(action, state_values[action]) for action in available_actions]
            max_value = max(q_values, key=lambda x: x[1])[1]
            max_actions = [action for action, value in q_values if value == max_value]
            return random.choice(max_actions)

    def learn(self, state, action, reward, next_state, done):
        old_value = self.q_table[state][action]
        next_max = np.max(self.q_table[next_state]) if not done else 0
        # Q-learning update rule
        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)
        self.q_table[state][action] = new_value

        # Decay the exploration rate
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

Explanation:

+ `__init__`: Initializes the Q-table and sets the hyperparameters for
  learning.
+ `choose_action`: Implements the ε-greedy policy for choosing
  actions.
    -  With probability $\epsilon$, the agent explores by selecting a
       random action.
    - Otherwise, it exploits by choosing the action with the highest
      estimated Q-value.
+ `learn`: Updates the Q-values based on the reward received and the
  maximum Q-value of the next state.


#### Define the Game Playing Function

We create a function to simulate a game between the agent and an
opponent.

```{python}
def play_game(agent, env, human_vs_agent=False):
    state = env.reset()
    done = False
    if human_vs_agent:
        print("Positions are as follows:")
        env.print_board_nums()
    current_player = 'X'  # Agent always plays 'X'

    while not done:
        if current_player == 'X':
            available_actions = env.available_actions()
            action = agent.choose_action(state, available_actions)
            env.make_move(action, 'X')
            next_state = env.get_state()
            if human_vs_agent:
                print("\nAgent's Move:")
                env.print_board()
            if env.current_winner == 'X':
                agent.learn(state, action, 1, next_state, True)
                if human_vs_agent:
                    print("Agent wins!")
                return 1  # Agent wins
            elif env.is_full():
                agent.learn(state, action, 0.5, next_state, True)
                if human_vs_agent:
                    print("It's a draw.")
                return 0.5  # Draw
            else:
                agent.learn(state, action, 0, next_state, False)
                state = next_state
                current_player = 'O'
        else:
            available_actions = env.available_actions()
            if human_vs_agent:
                valid_square = False
                while not valid_square:
                    user_input = input("Your move (0-8): ")
                    try:
                        action = int(user_input)
                        if action not in available_actions:
                            raise ValueError
                        valid_square = True
                    except ValueError:
                        print("Invalid move. Try again.")
                env.make_move(action, 'O')
                state = env.get_state()
            else:
                action = random.choice(available_actions)
                env.make_move(action, 'O')
            if env.current_winner == 'O':
                agent.learn(state, action, -1, env.get_state(), True)
                if human_vs_agent:
                    env.print_board()
                    print("You win!")
                return -1  # Agent loses
            elif env.is_full():
                agent.learn(state, action, 0.5, env.get_state(), True)
                if human_vs_agent:
                    print("It's a draw.")
                return 0.5  # Draw
            else:
                current_player = 'X'
```

Explanation:

+ Game Loop: Alternates turns between the agent and the opponent (or
  human player).
+ Agent's Turn:
    - Chooses an action using the ε-greedy policy.
	- Updates the Q-table based on the outcome.
+ Opponent's/Human's Turn:
    - If `human_vs_agent` is `True`, prompts the human for input.
	- Otherwise, the opponent makes a random move.
	- The agent updates its Q-table based on the outcome.
	
#### Define the Training Function

We define a function to train the agent over multiple episodes.

```{python}
def train_agent(episodes=50000):
    agent = QLearningAgent()
    env = TicTacToe()
    for episode in range(episodes):
        play_game(agent, env)
        if (episode + 1) % 10000 == 0:
            print(f"Episode {episode + 1}/{episodes} completed.")
    return agent
```

Explanation:

+ Initialization: Creates a new agent and game environment.
+ Training Loop: The agent plays the game repeatedly to learn from
  experience.
+ Progress Updates: Prints a message every 10,000 episodes to track
  training progress.


#### Define the Evaluation Function
We create a function to evaluate the agent's performance after
training.
```{python}
def evaluate_agent(agent, games=1000):
    env = TicTacToe()
    wins = 0
    draws = 0
    losses = 0
    for _ in range(games):
        result = play_game(agent, env)
        if result == 1:
            wins += 1
        elif result == 0.5:
            draws += 1
        else:
            losses += 1
    print(f"Out of {games} games: {wins} wins, {draws} draws, {losses} losses.")
```

Explanation:

+ Evaluation Loop: The agent plays a specified number of games without
  learning.
+ Outcome Tracking: Records the number of wins, draws, and losses.
+ Performance Display: Prints the results after evaluation.

#### Enable Human Interaction
We create a function to allow a human to play against the agent.

```{python}
def play_against_agent(agent):
    env = TicTacToe()
    play_game(agent, env, human_vs_agent=True)
```

#### Main Function
We define the main function to provide a user interface.
```{python}
def main():
    print("Tic-Tac-Toe with Reinforcement Learning Agent")
    print("1. Train Agent")
    print("2. Evaluate Agent")
    print("3. Play Against Agent")
    choice = input("Select an option (1-3): ")

    if choice == '1':
        episodes = int(input("Enter number of training episodes: "))
        agent = train_agent(episodes)
        # Save the trained agent
        import pickle
        with open('trained_agent.pkl', 'wb') as f:
            pickle.dump(agent, f)
        print("Agent trained and saved as 'trained_agent.pkl'.")
    elif choice == '2':
        # Load the trained agent
        import pickle
        try:
            with open('trained_agent.pkl', 'rb') as f:
                agent = pickle.load(f)
            evaluate_agent(agent)
        except FileNotFoundError:
            print("No trained agent found. Please train the agent first.")
    elif choice == '3':
        # Load the trained agent
        import pickle
        try:
            with open('trained_agent.pkl', 'rb') as f:
                agent = pickle.load(f)
            play_against_agent(agent)
        except FileNotFoundError:
            print("No trained agent found. Please train the agent first.")
    else:
        print("Invalid option selected.")
```


#### Example Session

Training and Evaluating the Agent:
```{pythopn}
agent = train_agent(50000)

evaluate_agent(agent, games=1000)
```

Playing Against the Agent:
```{python}
# play_against_agent(agent)
```








## Bias-Variance Tradeoff

The variance-bias trade-off is a core concept in machine learning that
explains the relationship between the complexity of a model, its
performance on training data, and its ability to generalize to unseen
data. It applies to both supervised and unsupervised learning, though
it manifests differently in each.


### Bias
Bias refers to the error introduced by approximating a complex
real-world problem with a simplified model. A model with high bias
makes strong assumptions about the data, leading to oversimplified
patterns and poor performance on both the training data and new
data. High bias results in underfitting, where the model fails to
capture important trends in the data.

+ Example (Supervised): In supervised learning, using a linear
  regression model to fit data that has a nonlinear relationship
  results in high bias because the model cannot capture the complexity
  of the data.
  
+ Example (Unsupervised): In clustering (an unsupervised task),
  setting the number of clusters too low (e.g., forcing data into two
  clusters when more exist) leads to high bias, as the model
  oversimplifies the underlying structure.
  
### Variance
Variance refers to the model's sensitivity to small changes in the
training data. A model with high variance will adapt closely to the
training data, potentially capturing noise or fluctuations that are
not representative of the general data distribution. High variance
leads to overfitting, where the model performs well on training data
but poorly on new, unseen data.


+ Example (Supervised): A decision tree with many branches can exhibit
  high variance. The model perfectly fits the training data but may
  perform poorly on test data because it overfits to specific
  idiosyncrasies in the training set.

+ Example (Unsupervised): In clustering, setting the number of
  clusters too high or fitting overly flexible cluster shapes (e.g.,
  in Gaussian Mixture Models) can lead to overfitting, where the model
  captures noise and splits data unnecessarily.

### The Trade-Off

The bias-variance trade-off reflects the tension between bias and
variance. As model complexity increases:

+ Bias decreases: The model becomes more flexible and can capture more
  details of the data.
+ Variance increases: The model becomes more sensitive to the
  particular training data, potentially capturing noise.
  
Conversely, a simpler model will:

+ Have high bias: It may not capture all relevant patterns in the
  data.
+ Have low variance: It will be less sensitive to fluctuations in the
  data and is more likely to generalize well to unseen data.

### Bias-Variance in Supervised Learning
In supervised learning, the goal is to strike the right balance
between bias and variance to minimize prediction error. This balance
is critical for developing models that generalize well to new
data. The total error of a model can be decomposed into:

$$
\text{Total Error} = \text{Bias}^2 + \text{Variance} 
+\text{Irreducible Error}.
$$

+ Bias: The error from using a model that is too simple.

+ Variance: The error from using a model that is too complex and
  overfits the training data.

+ Irreducible Error: This is the noise inherent in the data itself,
  which cannot be eliminated no matter how well the model is tuned.

### Bias-Variance in Unsupervised Learning
In unsupervised learning, the bias-variance trade-off is less formally
defined but still relevant. For example:


+ In clustering, choosing the wrong number of clusters can lead to
  either high bias (too few clusters, oversimplifying the data) or
  high variance (too many clusters, overfitting the data).

+ In dimensionality reduction, keeping too few components in Principal
  Component Analysis (PCA) increases bias by losing important
  information, while keeping too many components retains noise,
  increasing variance.

In unsupervised learning, balancing bias and variance typically
involves tuning hyperparameters (e.g., number of clusters, number of
components) to find the right complexity level.


### Striking the Right Balance
To strike the balance between bias and variance in both supervised and
unsupervised learning, techniques such as regularization, early
stopping, cross-validation, and hyperparameter tuning are
essential. These techniques help ensure the model is complex enough to
capture patterns in the data but not so complex that it overfits to
noise or irrelevant details.

## Crossvalidation

Cross-validation is a technique used to evaluate machine learning
models and tune hyperparameters by splitting the dataset into multiple
subsets. This approach helps to avoid overfitting and provides a
better estimate of the model's performance on unseen
data. Cross-validation is especially useful for managing the
bias-variance trade-off by allowing you to test how well the model
generalizes without relying on a single train-test split.


### K-Fold Cross-Validation

The most commonly used method is $k$-fold cross-validation:

+ Split the data: The dataset is divided into $k$ equally-sized folds
  (subsets).
+ Train on $k - 1$ folds: The model is trained on $k - 1$ folds,
  leaving one fold as a test set.
+ Test on the remaining fold: The model’s performance is evaluated on
  the fold that was left out.
+ Repeat: This process is repeated $k$ times, with each fold used once
  as the test set.
+ Average performance: The final cross-validation score is the average
  performance across all $k$ iterations.

By averaging the results across multiple test sets, cross-validation
provides a more robust estimate of model performance and helps avoid
overfitting or underfitting to any particular training-test split.


Leave-One-Out Cross-Validation (LOOCV) takes each observation as one
fold. The dataset is split into $n$ subsets (where $n$ is the sample
size), with each sample acting as a test set once. While this method
provides the most exhaustive evaluation, it can be computationally
expensive for large datasets.


### Benefits of Cross-Validation

+ Prevents overfitting: By testing the model on multiple subsets of
  data, cross-validation helps to identify if the model is too complex
  and overfits to the training data.
+ Prevents underfitting: If the model performs poorly across all
  folds, it may indicate that the model is too simple (high bias).
+ Better estimation: Cross-validation gives a better estimate of how
  the model will perform on unseen data compared to a single
  train-test split.

### Cross-Validation in Unsupervised Learning

While cross-validation is most commonly used in supervised learning,
it can also be applied to unsupervised learning through:


+ Stability testing: Running unsupervised algorithms (e.g.,
  clustering) on different data splits and measuring the stability of
  the results (e.g., using the silhouette score).
  
+ Internal validation metrics: In clustering, internal metrics like
  the silhouette score or Davies-Bouldin index can be used to evaluate
  the quality of clustering across different data splits.


The bias-variance trade-off is a universal problem in machine
learning, affecting both supervised and unsupervised
models. Cross-validation is a powerful tool for controlling this
trade-off by providing a reliable estimate of model performance and
helping to fine-tune model complexity. By balancing bias and variance
through careful model selection, regularization, and cross-validation,
you can develop models that generalize well to unseen data without
overfitting or underfitting.

### A Curve-Fitting with Splines: An Example


Overfitting occurs when a model becomes overly complex and starts to
capture not just the underlying patterns in the data but also the
noise or random fluctuations. This can lead to poor generalization to
new, unseen data. A clear sign of overfitting is when a model performs
very well on the training data but performs poorly on test data, as it
fails to generalize beyond the data it was trained on.


In this example, we illustrate overfitting using cubic spline
regression with different numbers of knots. Splines are a flexible
tool that allow for piecewise polynomial regression, with knots
defining where the pieces of the polynomial meet. The more knots we
use, the more flexible the model becomes, which can potentially lead
to overfitting.

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import SplineTransformer
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_squared_error

def true_function(X):
    return np.sin(1.5 * X) + 0.5 * np.cos(0.5 * X) + np.sin(2 * X)

# Generate synthetic data using the more complex true function
X = np.sort(np.random.rand(200) * 10).reshape(-1, 1)
y = true_function(X).ravel() + np.random.normal(0, 0.2, X.shape[0])

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Function to explore overfitting by plotting both errors and fitted curves
def overfitting_example_with_fitted_curves(X_train, y_train, X_test, y_test, knots_list):
    train_errors = []
    test_errors = []
    
    # Generate fine grid for plotting the true curve and fitted models
    X_line = np.linspace(0, 10, 1000).reshape(-1, 1)
    y_true = true_function(X_line)
    
    # Plot the true function and observed data
    plt.figure(figsize=(10, 6))
    plt.scatter(X_train, y_train, color='blue', label='Training data', alpha=0.6)
    plt.plot(X_line, y_true, label='True function', color='black', linestyle='--')
    
    for n_knots in knots_list:
        # Create a spline model with fixed degree = 3 (cubic) and varying knots
        spline = SplineTransformer(degree=3, n_knots=n_knots, include_bias=False)
        model = make_pipeline(spline, LinearRegression())
        
        # Fit the model to training data
        model.fit(X_train, y_train)
        
        # Predict on training and test data
        y_pred_train = model.predict(X_train)
        y_pred_test = model.predict(X_test)
        y_pred = model.predict(X_line)
        
        # Calculate training and test errors (mean squared error)
        train_errors.append(mean_squared_error(y_train, y_pred_train))
        test_errors.append(mean_squared_error(y_test, y_pred_test))
        
        # Plot the fitted curve
        plt.plot(X_line, y_pred, label=f'{n_knots} Knots (Fit)', alpha=0.7)

    print(train_errors, test_errors)
    
    plt.title('Overfitting Example: Fitted Curves and Observed Data')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.show()
    
    # Plot training and test error separately
    plt.figure(figsize=(10, 6))
    plt.plot(knots_list, train_errors, label='Training Error', marker='o', color='blue')
    plt.plot(knots_list, test_errors, label='Test Error', marker='o', color='red')
    plt.title('Training vs. Test Error with Varying Knots')
    plt.xlabel('Number of Knots')
    plt.ylabel('Mean Squared Error')
    plt.legend()
    plt.show()

# Explore overfitting by varying the number of knots and overlaying the fitted curves
knots_list = [6, 8, 10, 12, 14, 16]
overfitting_example_with_fitted_curves(X_train, y_train, X_test, y_test, knots_list)
```

In the first plot, we fit cubic splines with 2, 4, 6, 8, 10, and 12
knots to the data. As the number of knots increases, the model becomes
more flexible and better able to fit the training data. With only 2
knots, the model is quite smooth and underfits the data, capturing
only broad trends but missing the detailed structure of the true
underlying function. With 4 or 6 knots, the model begins to capture
more of the data’s structure, balancing the bias-variance trade-off
effectively. 
However, as we increase the number of knots to 10 and 12, the model
becomes too flexible. It starts to fit the noise in the training data,
producing a curve that adheres too closely to the data points. This is
a classic case of overfitting: the model fits the training data very
well, but it no longer generalizes to new data.


In the second plot, we observe the training error and test error as
the number of knots increases. As expected, the training error
consistently decreases as the number of knots increases, since a more
complex model can fit the training data better. However, the test
error tells a different story. Initially, the test error decreases as
the model becomes more flexible, indicating that the model is learning
meaningful patterns from the data. But after a certain point, the test
error begins to increase, signaling overfitting.


This is a key insight into the bias-variance trade-off. While adding
more complexity (in this case, more knots) reduces bias and improves
fit on the training data, it also increases variance, making the model
more sensitive to fluctuations and noise in the data. This results in
worse performance on test data, as the model becomes too specialized
to the training set.


The example clearly demonstrates how overfitting can occur when the
model becomes too complex. In practice, it's important to find a
balance between underfitting (high bias) and overfitting (high
variance). Techniques such as cross-validation, regularization, or
limiting model complexity (e.g., setting a reasonable number of knots
in spline regression) can help manage this trade-off and produce
models that generalize well to unseen data.


By tuning the number of knots or other model parameters, we can
achieve a model that strikes the right balance, capturing the true
patterns in the data without fitting the noise.

